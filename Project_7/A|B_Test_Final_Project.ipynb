{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.load_extensions('calico-spell-check')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%javascript\n",
    "#IPython.load_extensions('calico-spell-check')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#A/B Test Final Project -- Udacity Free Trial Screener    \n",
    "###Zach Farmer \n",
    "***   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Table of Contents   \n",
    "1. [Experiment Overview](#Experiment Overview)  \n",
    "2. [Experiment Design](#Experiment Design)   \n",
    "    * [Metric Choice](#Metric Choice)    \n",
    "    * [Measuring Standard Deviation](#Measuring Standard Deviation)   \n",
    "    * [Sizing](#Sizing)      \n",
    "3. [Experiment Analysis](#Experiment Analysis)     \n",
    "    * [Sanity Checks](#Sanity Checks)     \n",
    "    * [Result Analysis](#Result Analysis) \n",
    "    * [Recommendation](#Recommendation) \n",
    "4. [Follow-Up Experiment](#Follow-Up Experiment)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Experiment Overview\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Experiment Overview \n",
    "(excerpt from [Final Project Overview](https://docs.google.com/document/u/1/d/1aCquhIqsUApgsxQ8-SQBAigFDcfWVVohLEXcV6jWbdI/pub?embedded=True))       \n",
    "\n",
    "*At the time of this experiment, Udacity courses currently have two options on the home page: \"start free trial\", and \"access course materials\". If the student clicks \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.*\n",
    "\n",
    "*In the experiment, Udacity tested a change where if the student clicked \"start free trial\", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. [This screenshot](https://www.google.com/url?q=https://drive.google.com/a/knowlabs.com/file/d/0ByAfiG8HpNUMakVrS0s4cGN2TjQ/view?usp%3Dsharing&sa=D&usg=AFQjCNEGECoB6fS27szEfzRsLcsAWHVYkA) shows what the experiment looks like.*\n",
    "\n",
    "*The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough time without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.*    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Experiment Design\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Experiment Design      \n",
    "*The unit of diversion for this experiment is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Metric Choice\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Metric Choice   \n",
    "----\n",
    "\n",
    "**Invariant Metrics:**\n",
    "\n",
    "***Number of cookies:*** _Number of unique cookies<font color = \"red\">*</font> to view the course overview page._        \n",
    "* **Invariant Metric:** This is a population metric which is being measured before the free trial screener is reached, as a result it should not be affected by the unit of diversion split at the control and experiment stage. It should not be statistically different between the control and experiment.  \n",
    "\n",
    "***Number of clicks:*** *Number of unique cookies to click the “Start free trial” button (which happens before the free trial screener is triggered).*          \n",
    "* **Invariant Metric:**  This metric is being measured before the free trial screener is set to appear, it should not be affected by the experiment. If the unit of diversion is properly split then this value should be statistically no different for the control and experiment groups.\n",
    "   \n",
    "***Click-through-probability:*** *Number of unique cookie to click the “Start free trial” button divided by the number of unique cookies to view the course overview page.*         \n",
    "* **Invariant Metric:** This probability is measuring two different metrics neither of which should be affected by the experiment. The course overview page and the “Start free trial” button are two events that must occur before the free trial screener page can appear. Therefore this probability should be statistically the same between the control and experiment groups.      \n",
    "\n",
    "**Evaluation Metrics:** \n",
    "\n",
    "***Gross conversion:*** *Number of user-ids to complete checkout and enroll in the free trial divided by the number of unique cookies to click the “Start free trial” button.*  \n",
    "* **Evaluation Metric:** This metric should measure the effect of the free trial screener on those individuals who hit the “Start free trial” button, read the free trial screener pop-up and then decide if they wish to complete the checkout and enroll in the free trial. In other words if the free trial screener has the effect of altering what the current conversion rate is from “Start free trial” to actual enrollment in a statistically significant way this metric should capture that.    \n",
    "* **$d_{min}$ = 0.01**  *(minimum statistical significance boundary, the amount of change we require to see from a business perspective to be statistically practical to us.)* \n",
    "\n",
    "***Retention:*** *Number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of user-ids to complete checkout.*          \n",
    "* **Evaluation Metric:** This metric is an attempt to capture what effect the free trial screener had on altering the population of individuals signing up for the free trial and who subsequently remain enrolled. If the free trial screener has an effect then we should see a statistically significant change in the proportion of individuals who enroll for the free trial and those who remain enrolled after 14-days. This metric evaluates whether the rate of individuals who enrolled and then stay pass the free trial is affected by the free trial screener. Concretely are those who enrolled more likely to remain enrolled? This metric measures the goal Udacity would like to achieve with the free trial screener.      \n",
    "* **$d_{min}$ = 0.01** *(statistical significant boundary, the amount of change we require to see from a business perspective to be statistically practical to us.)*     \n",
    "\n",
    "***Net Conversion:*** *Number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the “Start free trial” button.*      \n",
    "* **Evaluation Metric:** This metric should measure what effect if any the free trial screener had on whether an individual who had watched the free trial screener decided to A: continue with enrollment and B: remained enrolled past 14 days.     \n",
    "* **$d_{min}$ = 0.0075** *(statistical significant boundary, the amount of change we require to see from a business perspective to be statistically practical to us.)*  \n",
    "\n",
    "The combination of the three evaluation metrics should determine whether Udacity’s hypothesis possesses validity. To consider launching the experiment we would want to see the gross conversion metric changing statistically significantly (by at least 1%). If the hypothesis is to prove valid then the change to the gross conversion metric will be negative as a result of users having better information about the time commitments involved in taking a course. Ideally this will result in less users making a commitment that they ultimately will not be able to keep, reducing the number of users who sign up for the free trial and giving up frustrated and instead choosing to “access course materials” for free. This will shrink the numerator and thus decrease the metric. \n",
    " \n",
    "We would like to see an increase in the retention rate, this would suggest that individuals who were interested in completing the course had better information regarding their ability to do so when they enrolled in the course originally and are more likely to continue on through the trial. This would manifest in a smaller denominator, a hopefully larger numerator and a subsequent increase in the retention rate metric.  \n",
    "\n",
    "Finally we would like to see no negative change in the net conversion. This would manifest itself by not seeing any statistically significant change in the negative direction for the net conversion metric. According to the hypothesis, *\"without significantly reducing the number of students to continue past the free trial and eventually complete the course.\"* If the effect of the free trial screener results in users who are more informed about time commitments we would hope to see a slight increase in the number of people who remain enrolled as they have a better understanding of the requirements and went into the trail more prepared. However at the very least we would not want our change to reduce the net conversion rate. \n",
    "\n",
    "\n",
    "\n",
    "> <font color=\"#c83400\" size=2>*(Any place \"unique cookies\" are mentioned, the uniqueness is determined by day. That is, the same cookie visiting on different days would be counted twice. User-ids are automatically unique since the site does not allow the same user-id to enroll twice.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Measuring Standard Deviation\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Measuring Standard Deviation    \n",
    "\n",
    "Using the provided [baseline values](https://docs.google.com/spreadsheets/d/1MYNUtC47Pg8hdoCjOXaHqF-thheGpUshrFA21BAJnNc/edit#gid=0) with a sample size of 5000 cookies visiting the course evaluation page, the following is the analytical estimate of standard deviation for each of the evaluation metrics.\n",
    "\n",
    "***Gross conversion:*** *(unit of analysis: # unique cookies to click “Start free trial” button)*      \n",
    "$p$ = .20625 (Probability of enrolling, given click)       \n",
    "$N$ = ( 5000(sample size) * .08(CTR on “Start free trial”) ) = 400 *(# unique cookies to click)*               \n",
    "$SE = \\sqrt{\\frac{p \\times (1-p)}{N}}$      \n",
    "$SE = \\sqrt{\\frac{.20625 \\times (1-.20625)}{400}}$ = 0.0202       \n",
    "* The unit of analysis in this case, the number of unique cookies to click the “Start free trial” button, is the same as the unit of diversion (cookies). Consequently I expect the analytical estimate to be comparable to the empirical estimate.       \n",
    "\n",
    "***Retention:*** *(unit of analysis: # user-ids to complete checkout)*      \n",
    "$p$ = 0.53 (Probability of payment, given enroll)        \n",
    "$N$ = ( 5000(sample size) * .08(CTR) * .20625(Prob. of enrolling given click) )= 82.5 *(number of enrollees)*      \n",
    "$SE = \\sqrt{\\frac{.53\\times(1-.53)}{82.5}}$ = 0.054949        \n",
    "* The unit of analysis in this metric is user-ids, as our unit of diversion for this A/B test is unique cookies this means that the unit of analysis and unit diversion are different. Therefore the analytical variability is liable to be different than the empirical variability. Often the analytical estimate, when the unit of analysis differs from that of the unit of diversion, is an underestimate and we would want to conduct an empirical estimate of variance.  \n",
    "\n",
    "\n",
    "***Net Conversion:*** *(unit of analysis: # unique cookies to click “Start free trial” button)*        \n",
    "$p$ = 0.1093125 (Probability of payment, given click)       \n",
    "$N$ = ( 5000(sample size) * .08(CTR on “Start free trial”) ) = 400 *(# unique cookies to click)*          \n",
    "$SE = \\sqrt{\\frac{.1093 \\times (1 - .1093)}{400}}$ = 0.0156        \n",
    "* The unit of analysis in this metric are cookies, this is the same unit as that of our unit of diversion. As a result I would posit that the empirical estimate will be very close to this analytical estimate.     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Sizing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Sizing        \n",
    "(sample sizes calculated using [evan millers sample size calculator](http://www.evanmiller.org/ab-testing/sample-size.html) )        \n",
    "###Number of Samples vs. Power\n",
    "\n",
    "If we assume independence between our metrics we can use the alpha = 0.05 for all three of our metrics, then the probability of seeing a false positive is: $P(FP \\ge 1) = 1 - .95^3$ = .143 or roughly 14%. If we were to use only two metrics the false positive probability would go down to roughly 9% ($1-.95^2$ = .0975).  \n",
    "\n",
    "Bonferroni’s correction is useful for determining the alpha levels of our metrics when we wish to make no assumptions about their independence and insure that the family wise error rate remains 5%. In the case of our three chosen metrics the individual alpha levels would equal $\\frac{.05}{3}$ = 0.01667 This would allows us to maintain a false positive rate no higher than 5%. However this has the effect of making the determination of significance more conservative, possibly failing to recognize actual significant differences at the .05 alpha level on an individual day by day basis. In addition we will also require more samples to boost power and achieve the higher confidence. \n",
    "\n",
    "***Gross Conversion:***     \n",
    "baseline conversion rate: 20.625%     \n",
    "minimum detectable effect: 1%    \n",
    "$\\beta$ = .02     \n",
    "$\\alpha$ = .05      \n",
    "result = 25,812 (# unique “Start free trial” cookies)      \n",
    "sample size for one group: 25,812 / 0.08(CTR prob.) = 322,650      \n",
    "sample size for control and experiment: 645,300      \n",
    "\n",
    "_For Bonferroni corrected $\\alpha$ = .01667      \n",
    "baseline conversion rate: 20.625%     \n",
    "minimum detectable effect: 1%     \n",
    "$\\beta$ = .02     \n",
    "$\\alpha$ = 0.01667 (for 3 metrics; .025 for 2)      \n",
    "result = 34,277 (# unique “Start free trial” cookies)      \n",
    "sample size for one group: 34,277/0.08(CTR prob.) = 428,462.5      \n",
    "sample size for control and experiment: 856,925_        \n",
    "\n",
    "***Retention:***          \n",
    "baseline conversion rate: 53%      \n",
    "minimum detectable effect: 1%     \n",
    "$\\beta$ = .02       \n",
    "$\\alpha$ = .05     \n",
    "result = 39,115 (# unique “Start free trial” cookies)      \n",
    "sample size for one group: 39,115/ (0.08(CTR prob.)*.20625(prob. enroll given click) = 2,370,606     \n",
    "sample size for control and experiment: 4,741,212     \n",
    "\n",
    "_For Bonferroni corrected alpha = .01667     \n",
    "baseline conversion rate: 53%     \n",
    "minimum detectable effect: 1%      \n",
    "$\\beta$ = .02     \n",
    "$\\alpha$ = 0.01667 (for 3 metrics; .025 for 2)      \n",
    "result = 52,155(# unique “Start free trial” cookies)      \n",
    "sample size for one group: 52,155/0.08(CTR prob.)*.20625(prob. enroll given click)  = 3,160,909.091       \n",
    "sample size for control and experiment: 6,321,818_      \n",
    "\n",
    "***Net Conversion:***                \n",
    "baseline conversion rate: 10.93125%      \n",
    "minimum detectable effect: .75%      \n",
    "$\\beta$ = .02        \n",
    "$\\alpha$ = .05      \n",
    "result = 27,413 (# unique “Start free trial” cookies)      \n",
    "sample size for one group: 27,413 / 0.08(CTR prob.) = 342,663     \n",
    "sample size for control and experiment: 685,325      \n",
    "\n",
    "_For Bonferroni corrected $\\alpha$ = .01667       \n",
    "baseline conversion rate: 10.93125%      \n",
    "minimum detectable effect: .75%      \n",
    "$\\beta$ = .02     \n",
    "$\\alpha$ = 0.01667 (for 3 metrics; .025 for 2)      \n",
    "result =  36241 (# unique “Start free trial” cookies)     \n",
    "sample size for one group: 36241/0.08(CTR prob.) = 453,013     \n",
    "sample size for control and experiment: 906,025_        \n",
    "\n",
    "\n",
    "If we assume independence between our metrics and calculate the sample sizes at an alpha level of 0.05 for each metric we would require a total 4,741,212 page-views for all three metrics. Retention's unit of analysis is user-id’s enrolled, for this reason we need a great many page-views in order to acquire enough enrolled user-ids to detect the desired minimum change of 1%. Given this fact and considering how long it would take to collect this many page-views even dedicating 100% (40000 page-views a day) of Udacity’s traffic, I will drop the retention metric and keep only the gross conversion and net conversion metrics. At this point net conversion requires the largest number of page-views for adequate statistical power, 685,325. \n",
    "\n",
    "If instead we use the Bonferroni correction we will lower the individual alphas and subsequently require more page-views to achieve minimum detectable change, 906,025. The net conversion metric requires the largest number of page-views at this point. The improved accuracy of using a Bonferroni correction on just 2 metrics might debatably be a value that can be traded off for shorter testing duration. The number of page-views required for the Bonferroni correction is roughly 20% higher than the number of page-views necessary without the correction and provides only a 4% reduction in the family wise error rate but translate into 3.4325 (4) more days at 100% traffic diversion for our experiment. Given the directions emphasis on requiring the experiment to run for no more then several weeks, I do not believe the correction to be the right choice in this instance. Therefore we will require the 685,325 page-views mentioned before to power our experiment. \n",
    "\n",
    "###Duration vs. Exposure  \n",
    "\n",
    "Assuming that this was the only experiment being run and we are using the baseline value of 40,000 page-views a day and that it is a high priority for Udacity to have the results, I would divert 90-100% of Udacity’s traffic to this experiment. I would justify this decision as follows; to begin with the risk to the user is small, meaning that no user will suffer more harm than they are currently suffering with the default. A user who intended to see the course through and had the time to do so will still enroll in the free trial, and those users who might have dropped out in frustration (owing to an under-appreciation for the time involved) will actually be better off. They will likely choose to access the free materials to begin with and will be spared the frustration of dropping the course as a result of realizing that they do not possess the necessary time each week to dedicate to a paid version of the course. \n",
    "\n",
    "There is a chance that Udacity itself may suffer some harm from a business perspective, there could exist a portion of the users who despite not really having enough time to complete the course may have signed up for the trial and stuck with it long enough to make payments. If all of these users were to no longer take the free trial course and instead were to access the free content immediately Udacity would lose that potential revenue. However It is doubtful that in the 18 days it would take to run this experiment that Udacity would suffer greatly from a financial point of view, additionally Udacity’s business plan likely revolves more around helping its user succeed in their learning goals rather than profiting from a miscalculation of course time demands by its users. A reduction in the net conversion metric would result in less \"sales\" and be a risk to Udacity's bottom line.   \n",
    "\n",
    "It is not likely there will be much in the way of user change aversion, as the user visible change is very minor, temporary and (universally) helpful. There is a risk that users may actually come to rely on the screener when making a determination about whether or not sign up for the free trial course. Not having a free trial screener available every time thereafter, a potential user deciding to enroll in a new course and expecting an estimation of the average time necessary to spend could be disappointed and frustrated if they did not have the screener. This might prevent potential enrollees from enrolling, I see this as the greatest potential harm to Udacity in deciding whether or not to run this experiment and how much traffic should see it.  \n",
    "\n",
    "If this experiment was not the only priority and the results could wait for a month or so then diverting 50 to 60% of the traffic would be a valid alternative to my initial decision. This might mitigate the potential harm caused by a learned effect of user choosing to sign up for a course only if they have an understanding of how much time it might take each week. Obviously reducing the percent of traffic is going to quickly extend the duration of our experiment, the cost of this needs to be weighed closely with the potential harm of allowing all of our users to see the traffic. \n",
    "\n",
    "* 100% traffic diversion: 18 days \n",
    "* 90% traffic diversion: 20 day\n",
    "* 60% traffic diversion: 29 days \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Experiment Analysis\"/>    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Experiment Analysis    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Sanity Checks\"/>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Sanity Checks        \n",
    "\n",
    "***Number of cookies:***      \n",
    "*(We can use a binomial distribution as there are only two events with equal probability of occurring)*      \n",
    "Probability = 0.5 ($p$)      \n",
    "Total # of cookies (control) = 345,543      \n",
    "Total # of cookies (experiment) = 344,660      \n",
    "Standard Error =$\\sqrt{\\frac{p\\times(1-p)}{N}}$ = $\\sqrt{\\frac{.5\\cdot.5}{345543+344660}}$ = 0.0006       \n",
    "Z-score = 1.96 (95% confidence)     \n",
    "Margin of error =$z \\times SE$ =  1.96 * 0.0006 = 0.00118      \n",
    "Confidence Interval:     \n",
    "Upper Bound = .50118     \n",
    "Lower Bound = .4988      \n",
    "Observed: $\\frac{\\text{# cookies (control)}}{\\text{# cookies (control)} + \\text{# cookies (experiment)}}$ = $\\frac{345543}{345543 + 344660}$ = .50064        \n",
    "* Passes sanity check ?: Yes this is within the upper bound of the 95% confidence interval \n",
    "    \n",
    "***Number of clicks:***       \n",
    "*(We can use a binomial distribution as there are only two events with equal probability of occurring)*     \n",
    "Probability = .5 ($p$)        \n",
    "Total # of unique cookies to click “Start free trial” (control)  = 28,378      \n",
    "Total # of unique cookies to click “Start free trial” (experiment) = 28,325       \n",
    "Standard Error =  $\\sqrt{\\frac{p\\cdot(1-p)}{N}}$ = $\\sqrt{\\frac{.5\\cdot.5}{28378+28325}}$ = .0021     \n",
    "Z-score = 1.96 (95% confidence)          \n",
    "Margin of error = $z \\times SE$ = 1.96 * .0021 = .0041155     \n",
    "Confidence Interval:     \n",
    "Upper Bound = .50411        \n",
    "Lower Bound = .49588      \n",
    "Observed:$\\frac{\\text{# unique cookie clicks (control)}} {\\text{# unique cookie clicks (control)} + \\text{# unique cookie clicks (experiment)}}$ = $\\frac{28378}{28378+28325}$ = .50047      \n",
    "* Passes sanity check ?: Yes this metric passes the sanity check, there does not exist a significant difference between the control and experiment groups’ unique cookies that click on the “Start free trial” button.          \n",
    "\n",
    "***Click-through-probability:***     \n",
    "$\\hat p$ = $\\frac{\\text{# unique cookies to click “Start free trial” (control)}}{\\text{# of unique page-view cookies (control)}}$ = 0.082126          \n",
    "Total # of page-views (control) = 345543         \n",
    "Standard Error: $\\sqrt{\\frac{\\hat p\\cdot(1-\\hat p)}{N}}$ = $\\sqrt{\\frac{.0821\\cdot(1-.0821)}{345543}}$ = .000467    \n",
    "Z-score = 1.96 (95% confidence)       \n",
    "margin of error = $z \\times SE$= 1.96 * .000467 = .00091532     \n",
    "Confidence Interval:     \n",
    "Upper Bound = .08304132      \n",
    "Lower Bound = .08121068      \n",
    "Observed: $\\frac{\\text{# unique cookies to click “Start free trial” (experiment)}}{\\text{# of unique page-view cookies (experiment)}}$ = 0.08218244067      \n",
    "* Passes sanity check ?: Yes the CTP (click-through-probability) for the experiment falls within the upper bounds of the 95% confidence interval of the CTP for the control group.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Result Analysis\"/>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Result Analysis      \n",
    "\n",
    "###Effect Size Tests       \n",
    "***Gross Conversion:*** *(Number of user-ids to complete checkout and enroll in the free trial divided by the number of unique cookies to click the “Start free trial” button.)*       \n",
    "$d_{min}$ = 0.01      \n",
    "$N_{control}$ = 17293     \n",
    "$N_{experiment}$ = 17260        \n",
    "$\\hat p_{control}$ = $\\frac{\\text{# unique users to enroll}}{\\text{# unique cookies to click “Start free trial”}}$ = 0.2188746892       \n",
    "$\\hat p_{experiment}$ = $\\frac{\\text{# unique users to enroll}}{\\text{# unique cookies to click “Start free trial”}}$ = 0.1983198146     \n",
    "$\\hat p_{pooled}$ = $\\frac{\\text{# unique user to enroll (control)} + \\text{# unique users to enroll (experiment)}}{\\text{# unique cookies to click (control)} + \\text{# unique cookies to click (experiment)}}$ = 0.2086070674       \n",
    "Standard Error Pooled = $\\sqrt{\\hat p_{pooled}\\cdot(1-\\hat p_{pooled}) * \\frac{1}{N_{control}} + \\frac{1}{N_{experiment}})}$ = $\\sqrt{.2086 \\cdot(1-.2086) * ( \\frac{1}{17293} + \\frac{1}{17260})}$ = .00437         \n",
    "$\\hat d$ (difference) = $\\hat p_{experiment} - \\hat p_{control}$ = .1983198146 - .2188746892 = -.0205548746    \n",
    "margin of error = $z \\times SE$   = 1.96 * 0.00437 = .0085652      \n",
    "Confidence Interval:    \n",
    "Upper bound = -.0119896746     \n",
    "Lower bound = -.0291200746        \n",
    "Statistical Significant ?: Yes, the confidence interval does not contain 0.     \n",
    "Practical significance ?: Yes, The absolute difference is greater than 0.01 or 1% in the negative direction.         \n",
    "\n",
    "***Net Conversion:*** *(Number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the “Start free trial” button.)*      \n",
    "$d_{min}$ = 0.0075      \n",
    "$N_{control}$ = 17293       \n",
    "$N_{experiment}$ = 17260      \n",
    "$\\hat p_{control}$ = $\\frac{\\text{# unique users to pay}}{\\text{# unique cookies to click “Start free trial”}}$ = 0.1175620193     \n",
    "$\\hat p_{experiment}$ = $\\frac{\\text{# unique users to pay}}{\\text{# unique cookies to click “Start free trial”}}$ = 0.1126882966     \n",
    "$\\hat p_{pooled}$ = $\\frac{\\text{# unique user to pay (control)} + \\text{# unique users to pay (experiment)}}{\\text{# unique cookies to click (control)} + \\text{# unique cookies to click (experiment)}}$ = 0.1151274853      \n",
    "Standard Error Pooled = $\\sqrt{\\hat p_{pooled}\\cdot(1-\\hat p_{pooled}) * (\\frac{1}{N_{control}} + \\frac{1}{N_{exp}})}$ = $\\sqrt{.115\\cdot(1-.115) *( \\frac{1}{17293} + \\frac{1}{17260})}$= .0034341272        \n",
    "$\\hat d$ (difference) = $\\hat p_{experiment} - \\hat p_{control}$ = .1126882966 - .1175620193 = -.0048737227       \n",
    "margin of error = $z \\times SE$ = 1.96 * .003431272 = .0067252931     \n",
    "Confidence Interval:    \n",
    "Upper bound = .0018515704      \n",
    "Lower bound = -.0115990158      \n",
    "Statistical Significant ?: No, Includes 0 in the confidence interval        \n",
    "Practical significance ?: No, the $d_{min}$ is larger than one of the bounds and the interval includes zero. A portion of the interval is greater than the $d_{min}$ in the negative direction but unless we can narrow the confidence interval to so that both upper and lower lie outside the minimum significance boundary we cannot state with confidence that the experiment has a statistically practical significance.   \n",
    "\n",
    "###Sign Tests\n",
    "\n",
    "***Gross Conversion:*** *(Number of user-ids to complete checkout and enroll in the free trial divided by the number of unique cookies to click the “Start free trial” button.)*       \n",
    "p-value: 0.0026 (two-tailed)               \n",
    "Statistically Significant ?: Yes. the p-value is less than .05     \n",
    "\n",
    "***Net Conversion:*** *(Number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the “Start free trial” button.)*      \n",
    "p-value: 0.6776 (two-tailed)     \n",
    "Statistically Significant ?: No     \n",
    "\n",
    "###Summary\n",
    "\n",
    "It appears that the effect size results and the sign test agree with each other. I decided not to use the Bonferroni correction after calculating its potential benefit, a 4% decrease in the false positive rate and its cost, a sample size increase of 20%. Given the nominal benefits and the not insignificant amount of additional time the experiment would have to run to power the Bonferroni's increased confidence, I decided that for the purpose of this analysis to not employ the correction. If the addition of another 4 days and higher confidence was deemed as more important than test duration I would reverse my decision on the Bonferroni correction. \n",
    "\n",
    "Clearly the retention metric, out of the three metrics, comes closest to measuring the effect of Udacity's goal. However as discussed before and likely a result of other business constraints, the time required to accurately power this metric is not a viable option. The best that we can accomplish with the last two metrics is an approximation on whether the goal is being achieved. The results of the analysis do seem to indicate some progress in the direction that Udacity was hoping for with no regression in the net conversion rate which was another key measure of success. We can surmise therefore that the combination of these two metrics is likely telling us that the retention rate, had we measured it, would have shown a positive change. We can make this hypothesis because we know that no significant difference occurred in the net conversion rate, meaning similar numbers of users were making it through to payment from all those to click the \"Start free trial\" button. Combined with the gross retention rate which was telling us that fewer users were moving past the checkout stage to enrollment from all those who started the process, indicating that the denominator in the retention metric would shrink while the numerator would remain constant and thus the rate would increase. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Recommendation\"/>      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Recommendation          \n",
    "Based on the results of two metrics measured I would recommend the free trial screener as a change worth implementing. We saw a statistically significant and practically significant change in the gross conversion rate, implying a reduction in the number of drop-outs. Even though we did not see a statistically significant increase in the number of users enrolling in the net conversion, it did not decrease as a result of the change which was one of constraints Udacity placed on the implementation of this change. This means that while no less students are making it through the trial, less were starting the trial to begin with. Thus the goal of less students dropping out of the free trial from those users who enrolled has been met. Therefore I would recommend launching this change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"Follow-Up Experiment\"/>         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Follow-Up Experiment: How to Reduce Early Cancellations   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you wanted to reduce the number of frustrated students who cancel early in the course, what experiment would you try? Give a brief description of the change you would make, what your hypothesis would be about the effect of the change, what metrics you would want to measure, and what unit of diversion you would use. Include an explanation of each of your choices.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Udacity's experiment above works to address frustrated students dropping out as a result of improperly accounting for the necessary time involved. This is certainly one valid way in which potential students may become frustrated. There are other reasons a student may be frustrated and I'd like to explore one of them. The feeling of struggling with or being confused about the course content. These feelings can be exacerbated by believing you are the only one struggling with the content or by feeling lost in how to approach the content. I'm not sure how many students view or use the forums or available coaching but an attempt to reduce frustrated students could be implemented after the course sign up but before early cancellation. I'd be curious to see if at the end week the top forum questions were emailed out to an experiment group of course participants we might not find a decrease in frustrated students dropping out. This email would include the top (ten?) forum questions related to the lesson the user was currently studying and perhaps also include links to one-on-one coaching resources and a general link to the course forum. My underlying theory is that a portion of those users dropping out are doing so as a result of finding the content more challenging then they initially thought and perhaps operating under the illusion that they are unique in this feeling and deciding that they are not prepared to take the course when this may not be the case. I am also assuming from personal experience that some of these frustrated users might have forgotten about the resources available to them to address their struggles. Finally I am trusting to the psychological benefit of knowing that you are not the only one struggling with the content and that many others are facing similar challenges as motivation to push forward and not quite when the going gets rough.  \n",
    "\n",
    "The goal of the experiment is to see an decrease in the number of people dropping out after signing up for and starting the class. Certainly we do not want to see an increase in early cancellation and no change in early cancellation would suggest no difference was made by the experiment. I would hypothesis a slight decrease in the number of 'frustrated' students canceling the course early. In order to implement this experiment several metrics would have to be monitored. Obviously we would need to know what users were associated with what classes. Importantly too we would need to record the number of user-ids associated with each question in the forums related to the class and lesson and record these. The user-id who created the question and all the user-ids to 'read' (click on and stay for longer the a couple of seconds) or comment on the questions would need to be counted. Recording these metrics would be necessary simply to affect the experiment itself. In terms of measuring the outcome of the experiment we are implementing this experiment after sign-up, therefore we will be monitoring user-ids. This means that we should be measuring retention using user-id, user-ids who sign up and user-ids that cancel early (early could be before the first payment or before the estimated amount of time to finish the course). We should be able to use the same invariant metrics as before and include gross conversion as an invariant because we should only be effecting users who have already gone through the entire process of signing up for a course.        \n",
    "For our invariants we should see no statistical change between the experiment and control groups when we look at the number of unique cookies to visit the Udacity site. We should see no change in the number of clicks on the start free trial button and no difference in the click-through-probability of the previous two metrics. Finally we should see no difference between the two groups in the gross conversion rate, as the experimental change should not occur until after sign up has been completed.     \n",
    "\n",
    "The unit of diversion should be user-ids. Our evaluation metrics of retention (for whatever length of time we deem as early into the course) should if the hypothesis holds water increase, same for the net conversion rate which should increase if the hypothesis is valid. The unit of diversion would need to be user-id, we implement this change only for students enrolled in a course and the experimental change is unique to the course itself. This type of study however would likely require a good deal of time to properly power as the number of people signing up for classes is not nearly as large as the number of users visiting Udacity's homepage. What we are focused on here is preventing those users who already signed up from canceling early, this means that the the diversion of traffic is splitting on a very specific subset of Udacity's traffic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
